# Подключение к API (дополнение к шаблону)

Если вы расширяете портфолио-шаблон чат-ботом или формой с отправкой на сервер, полезно держать в голове общий путь интеграции с внешними API. Это не обязательная часть шаблона — дополнение для воркшопов и обучения.

## Как искали подход к API (из опыта проекта)

При интеграции чат-бота рассматривали несколько провайдеров LLM API. Сами по себе Gemini и Grok не плохие; ниже — почему в этом проекте пошли дальше и остановились на OpenAI.

- **Gemini (Google):** Есть распространённое мнение, что модель по качеству/контексту слабее альтернатив для сценария «жёсткая база знаний в промпте». Плюс на проде быстро упирались в квоты бесплатного тарифа. Решили рассмотреть другие варианты.
- **Grok (xAI):** Бесплатного trial по факту не дали — для доступа к API запросили оплату. Попытка оплатить небольшую сумму не прошла (оплату завершить не удалось; детали платёжной цепочки не раскрываем). В итоге доступ к API получить не удалось.
- **OpenAI:** При сопоставимых затратах (или в рамках того, что готовы платить за токены) оказался рабочей вариацией: доступ к API и привязка/оплата прошли без проблем, модель с достаточным контекстом и лимитами (например 200k TPM). Один endpoint, один ключ в переменных окружения — фронт при смене провайдера не меняли.

Итог: Gemini и Grok не дискредитированы — для тех же денег и в нашей ситуации OpenAI оказался тем вариантом, который удалось довести до работы. Подход «один backend-endpoint + смена провайдера в одном месте» позволил переключаться без переписывания фронта. В README своего проекта можно кратко описать, какие варианты пробовали и почему остановились на текущем — без чувствительных деталей (платежи, адреса, аккаунты).

## Общие принципы

1. **Секреты не в коде.** API-ключи и токены задавайте только в переменных окружения (например в Vercel: Settings → Environment Variables). В репозитории — только описание, какая переменная нужна (например `OPENAI_API_KEY`).
2. **Один endpoint — один провайдер.** Backend в виде одной serverless-функции (например `api/chat.js` или `api/send.js`) упрощает смену провайдера: меняете URL, формат запроса и разбор ответа в одном месте.
3. **Фронт от API не зависит.** Виджет чата или форма шлют POST на ваш endpoint; что происходит на сервере (какой LLM или почтовый сервис) — деталь реализации.

## Чат-бот с LLM

- **Фронт:** виджет (кнопка, окно, поле ввода). По отправке сообщения — POST на `/api/chat` с телом `{ message: "..." }`. Отображаете ответ из `response` или сообщение об ошибке из `error`.
- **Backend (Vercel Function):** читаете `message`, подставляете системный промпт (база знаний), вызываете API выбранного провайдера (OpenAI, Gemini, Grok и т.д.), возвращаете `{ response: "..." }`. Модель не дообучаете — только промпт; так экономятся токены.
- **Смена провайдера:** правка одной функции и переменной окружения; фронт не трогаете.

## Форма обратной связи

- **Вариант A (без бэкенда):** валидация на клиенте, затем `mailto:?subject=...&body=...`. Письмо отправится только когда пользователь нажмёт «Отправить» в своём почтовом клиенте.
- **Вариант B (с бэкендом):** форма шлёт POST на ваш endpoint (например `/api/send`); функция отправляет письмо через сервис (Resend, SendGrid и т.п.) и возвращает успех/ошибку. Нужен API-ключ сервиса в переменных окружения.

## Состояние чат-бота и база знаний (инструкция для людей)

Чат-бот в таком шаблоне/проекте **сознательно** сделан в состоянии «слабо обучен»: у него небольшая, жёстко заданная база знаний в системном промпте (одна константа в коде backend). Модель **не дообучают** — не тратят токены на fine-tuning и не подмешивают большие объёмы данных в каждый запрос. Поэтому бот может казаться «туповатым» или «слабым по контексту»: он отвечает только на основе короткого текста, который вы положили в промпт.

**Это не баг.** Так задумано для экономии и простоты. Если нужно сделать бота умнее и точнее:

1. **Расширить системный промпт (жёсткая база знаний).** В backend-функции увеличьте константу с инструкциями и фактами: добавьте типовые вопросы–ответы, цены, правила, ограничения. Модель будет опираться на этот текст при каждом запросе. Подходит, пока объём умещается в лимит контекста и не раздувает стоимость запроса.
2. **Вынести базу в отдельный документ/файл.** Храните текст базы знаний в файле (Markdown, JSON) или в CMS; при старте функции или по запросу подставляйте его в системное сообщение. Так проще править контент без правки кода.
3. **RAG (поиск по базе перед ответом).** Если фактов много: при запросе пользователя сначала ищете релевантные фрагменты (поиск по эмбеддингам или по ключевым словам), подставляете только их в промпт. Нужна отдельная хранилище и, при желании, эмбеддинги — это уже следующий уровень по сравнению с «одна константа в коде».

Кратко для воркшопа и для разработчиков: бот «слабый» по задумке; чтобы его «научить», начните с расширения системного промпта (жёсткая база знаний), при необходимости переходите к выносу базы в файл или к RAG.

## Документация в проекте

В README проекта укажите: какие переменные окружения нужны (имена), для чего они, и куда пользователь может добавить ключи (Vercel, .env при локальной разработке). Без значений ключей — только плейсхолдеры в примерах.
